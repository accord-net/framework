<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Accord.Vision</name>
    </assembly>
    <members>
        <member name="T:Accord.Vision.GroupMatching">
            <summary>
              Group matching algorithm for detection region averaging.
            </summary>
            
            <remarks>
              This class can be seen as a post-processing filter. Its goal is to
              group near or contained regions together in order to produce more
              robust and smooth estimates of the detected regions.
            </remarks>
            
        </member>
        <member name="T:Accord.Vision.GroupMatching`1">
            <summary>
              Group matching algorithm for detection region averaging.
            </summary>
            
            <remarks>
              This class can be seen as a post-processing filter. Its goal is to
              group near or contained regions together in order to produce more
              robust and smooth estimates of the detected regions.
            </remarks>
            
        </member>
        <member name="M:Accord.Vision.GroupMatching`1.#ctor(System.Int32)">
            <summary>
              Creates a new <see cref="T:Accord.Vision.GroupMatching"/> object.
            </summary>
            
            <param name="minimumNeighbors">
              The minimum number of neighbors needed to keep a detection. If a rectangle
              has less than this minimum number, it will be discarded as a false positive.</param>
            
        </member>
        <member name="M:Accord.Vision.GroupMatching`1.Group(`0[])">
            <summary>
              Groups possibly near rectangles into a smaller
              set of distinct and averaged rectangles.
            </summary>
            
            <param name="shapes">The rectangles to group.</param>
            
        </member>
        <member name="M:Accord.Vision.GroupMatching`1.classify(`0[])">
            <summary>
              Detects rectangles which are near and 
              assigns similar class labels accordingly.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.GroupMatching`1.merge(System.Int32,System.Int32)">
            <summary>
              Merges two labels.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.GroupMatching`1.Near(`0,`0)">
            <summary>
              When overridden in a child class, should compute
              whether two given shapes are near. Definition of
              near is up to the implementation.
            </summary>
            
            <returns>True if the two shapes are near; false otherwise.</returns>
            
        </member>
        <member name="M:Accord.Vision.GroupMatching`1.Average(System.Int32[],`0[],System.Int32[]@)">
            <summary>
              When overridden in a child class, should compute
              an average of the shapes given as parameters.
            </summary>
            
            <param name="labels">The label of each shape.</param>
            <param name="shapes">The shapes themselves.</param>
            <param name="neighborCounts">Should return how many neighbors each shape had.</param>
            
            <returns>The averaged shapes found in the given parameters.</returns>
            
        </member>
        <member name="P:Accord.Vision.GroupMatching`1.MinimumNeighbors">
            <summary>
              Gets or sets the minimum number of neighbors necessary to keep a detection.
              If a rectangle has less neighbors than this number, it will be discarded as
              a false positive.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.GroupMatching`1.Classes">
            <summary>
              Gets how many classes were found in the
              last call to <see cref="M:Accord.Vision.GroupMatching`1.Group(`0[])"/>.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.GroupMatching.#ctor(System.Int32,System.Double)">
            <summary>
              Creates a new <see cref="T:Accord.Vision.GroupMatching"/> object.
            </summary>
            
            <param name="minimumNeighbors">
              The minimum number of neighbors needed to keep a detection. If a rectangle
              has less than this minimum number, it will be discarded as a false positive.</param>
            <param name="threshold">
              The minimum distance threshold to consider two rectangles as neighbors.
              Default is 0.2.</param>
            
        </member>
        <member name="M:Accord.Vision.GroupMatching.Near(System.Drawing.Rectangle,System.Drawing.Rectangle)">
            <summary>
              Checks if two rectangles are near.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.GroupMatching.Average(System.Int32[],System.Drawing.Rectangle[],System.Int32[]@)">
            <summary>
              Averages rectangles which belongs to the
              same class (have the same class label)
            </summary>
            
        </member>
        <member name="P:Accord.Vision.GroupMatching.Threshold">
            <summary>
              Gets the minimum distance threshold to consider
              two rectangles as neighbors. Default is 0.2.
            </summary>
            
        </member>
        <member name="T:Accord.Vision.Detection.Cascades.FaceHaarCascade">
            <summary>
              Default Face Haar Cascade for using with Haar Classifiers.
            </summary>
            
            <remarks>
              The definition was originally based on a hard coded partial transcription of
              OpenCV's <i>haarcascade_frontalface_alt.xml</i> by Mario Klingemann. This
              class, however, has been re-created using <see cref="T:Accord.Vision.Detection.HaarCascadeWriter"/>.
            </remarks>
            
        </member>
        <member name="T:Accord.Vision.Detection.HaarCascade">
            <summary>
              Cascade of Haar-like features' weak classification stages.
            </summary>
            
            <remarks>
            <para>
              The Viola-Jones object detection framework is the first object detection framework
              to provide competitive object detection rates in real-time proposed in 2001 by Paul
              Viola and Michael Jones. Although it can be trained to detect a variety of object
              classes, it was motivated primarily by the problem of face detection.</para>
              
            <para>
              The implementation of this code has used Viola and Jones' original publication, the
              OpenCV Library and the Marilena Project as reference. OpenCV is released under a BSD
              license, it is free for both academic and commercial use. Please be aware that some
              particular versions of the Haar object detection framework are patented by Viola and
              Jones and may be subject to restrictions for use in commercial applications. </para>
              
             <para>
                References:
                <list type="bullet">
                  <item><description>
                    <a href="http://www.cs.utexas.edu/~grauman/courses/spring2007/395T/papers/viola_cvpr2001.pdf">
                    Viola, P. and Jones, M. (2001). Rapid Object Detection using a Boosted Cascade
                    of Simple Features.</a></description></item>
                  <item><description>
                    <a href="http://en.wikipedia.org/wiki/Viola-Jones_object_detection_framework">
                    Wikipedia, The Free Encyclopedia. Viola-Jones object detection framework </a>
                  </description></item>
                </list></para>
            </remarks>
            
            <example>
            <para>
              To load an OpenCV-compatible XML definition for a Haar cascade, you can use HaarCascade's
              <see cref="M:Accord.Vision.Detection.HaarCascade.FromXml(System.IO.Stream)">FromXml</see> static method. An example would be:</para>
              <code>
              String path = @"C:\Users\haarcascade-frontalface_alt2.xml";
              HaarCascade cascade1 = HaarCascade.FromXml(path);
              </code>
              
            <para>
              After the cascade has been loaded, it is possible to create a new <see cref="T:Accord.Vision.Detection.HaarObjectDetector"/>
              using the cascade. Please see <see cref="T:Accord.Vision.Detection.HaarObjectDetector"/> for more details. It is also
              possible to generate embeddable C# definitions from a cascade, avoiding the need to load
              XML files on program startup. Please see <see cref="M:Accord.Vision.Detection.HaarCascade.ToCode(System.String,System.String)"/> method or 
              <see cref="T:Accord.Vision.Detection.HaarCascadeWriter"/> class for details.</para> 
            </example>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarCascade.#ctor(System.Int32,System.Int32,Accord.Vision.Detection.HaarCascadeStage[])">
            <summary>
              Constructs a new Haar Cascade.
            </summary>
            
            <param name="baseWidth">Base feature width.</param>
            <param name="baseHeight">Base feature height.</param>
            <param name="stages">Haar-like features classification stages.</param>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarCascade.#ctor(System.Int32,System.Int32)">
            <summary>
              Constructs a new Haar Cascade.
            </summary>
            
            <param name="baseWidth">Base feature width.</param>
            <param name="baseHeight">Base feature height.</param>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarCascade.checkTiltedFeatures(Accord.Vision.Detection.HaarCascadeStage[])">
            <summary>
              Checks if the classifier contains tilted (rotated) features
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarCascade.Clone">
            <summary>
              Creates a new object that is a copy of the current instance.
            </summary>
            
            <returns>
              A new object that is a copy of this instance.
            </returns>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarCascade.FromXml(System.IO.Stream)">
            <summary>
              Loads a HaarCascade from a OpenCV-compatible XML file.
            </summary>
            
            <param name="stream">
               A <see cref="T:System.IO.Stream"/> containing the file stream
               for the xml definition of the classifier to be loaded.</param>
               
            <returns>The HaarCascadeClassifier loaded from the file.</returns>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarCascade.FromXml(System.String)">
            <summary>
              Loads a HaarCascade from a OpenCV-compatible XML file.
            </summary>
            
            <param name="path">
               The file path for the xml definition of the classifier to be loaded.</param>
               
            <returns>The HaarCascadeClassifier loaded from the file.</returns>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarCascade.FromXml(System.IO.TextReader)">
            <summary>
              Loads a HaarCascade from a OpenCV-compatible XML file.
            </summary>
            
            <param name="stringReader">
               A <see cref="T:System.IO.StringReader"/> containing the file stream
               for the xml definition of the classifier to be loaded.</param>
               
            <returns>The HaarCascadeClassifier loaded from the file.</returns>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarCascade.ToCode(System.String,System.String)">
            <summary>
              Saves a HaarCascade to C# code.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarCascade.ToCode(System.IO.TextWriter,System.String)">
            <summary>
              Saves a HaarCascade to C# code.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarCascade.Width">
            <summary>
              Gets the stages' base width.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarCascade.Height">
            <summary>
              Gets the stages' base height.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarCascade.Stages">
            <summary>
              Gets the classification stages.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarCascade.HasTiltedFeatures">
            <summary>
              Gets a value indicating whether this cascade has tilted features.
            </summary>
            
            <value>
            	<c>true</c> if this cascade has tilted features; otherwise, <c>false</c>.
            </value>
            
        </member>
        <member name="M:Accord.Vision.Detection.Cascades.FaceHaarCascade.#ctor">
            <summary>
              Hard-coded partial transcription of <i>haarcascade_frontalface_alt.xml</i>
              based on code by Mario Klingemann.
            </summary>
            
        </member>
        <member name="T:Accord.Vision.Detection.Cascades.NoseHaarCascade">
            <summary>
              Automatic transcription of Haar cascade definitions 
              for facial features by Modesto Castrillon-Santana.
            </summary>
            
            <remarks>
            <para>
              This code has been automatically generated by the Accord.NET Framework
              based on original research by Modesto Castrillon-Santana. The original
              code has been shared under a BSD license in the OpenCV library and has
              been incorporated in the Accord.NET Framework under permission of the
              original author.</para>
              
            <code>
              // Copyright (c) 2008, Modesto Castrillon-Santana (IUSIANI, University of
              // Las Palmas de Gran Canaria, Spain).
              //  All rights reserved.
              //  
              // Redistribution and use in source and binary forms, with or without
              // modification, are permitted provided that the following conditions are
              // met:
              //       
              //    * Redistributions of source code must retain the above copyright
              //      notice, this list of conditions and the following disclaimer.
              //    * Redistributions in binary form must reproduce the above
              //      copyright notice, this list of conditions and the following
              //      disclaimer in the documentation and/or other materials provided
              //      with the distribution.  
              //    * The name of Contributor may not used to endorse or promote products 
              //      derived from this software without specific prior written permission.
              //
              //  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
              //  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
              //  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
              //  A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
              //  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
              //  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
              //  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
              //  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
              //  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
              //  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
              //  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
            </code>
            
            </remarks>
            
        </member>
        <member name="M:Accord.Vision.Detection.Cascades.NoseHaarCascade.#ctor">
            <summary>
              Creates a new cascade for nose detection.
            </summary>
            
        </member>
        <member name="T:Accord.Vision.Detection.HaarCascadeWriter">
            <summary>
              Automatic transcriber for Haar cascades.
            </summary>
            
            <remarks>
              This class can be used to generate code-only definitions for Haar cascades,
              avoiding the need for loading and parsing XML files during application startup.
              This class generates C# code for a class inheriting from <see cref="T:Accord.Vision.Detection.HaarCascade"/>
              which may be used to create a <see cref="T:Accord.Vision.Detection.HaarObjectDetector"/>.
            </remarks>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarCascadeWriter.#ctor(System.IO.TextWriter)">
            <summary>
              Constructs a new <see cref="T:Accord.Vision.Detection.HaarCascadeWriter"/> class.
            </summary>
            <param name="stream">The stream to write to.</param>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarCascadeWriter.Write(Accord.Vision.Detection.HaarCascade,System.String)">
            <summary>
              Writes the specified cascade.
            </summary>
            <param name="cascade">The cascade to write.</param>
            <param name="className">The name for the generated class.</param>
            
        </member>
        <member name="T:Accord.Vision.Detection.HaarClassifier">
             <summary>
               Strong classifier based on a weaker cascade of
               classifiers using Haar-like rectangular features.
             </summary>
            
             <remarks>
             <para>
               The Viola-Jones object detection framework is the first object detection framework
               to provide competitive object detection rates in real-time proposed in 2001 by Paul
               Viola and Michael Jones. Although it can be trained to detect a variety of object
               classes, it was motivated primarily by the problem of face detection.</para>
               
             <para>
               The implementation of this code has used Viola and Jones' original publication, the
               OpenCV Library and the Marilena Project as reference. OpenCV is released under a BSD
               license, it is free for both academic and commercial use. Please be aware that some
               particular versions of the Haar object detection framework are patented by Viola and
               Jones and may be subject to restrictions for use in commercial applications. The code
               has been implemented with full support for tilted Haar features.</para>
               
              <para>
                 References:
                 <list type="bullet">
                   <item><description>
                     <a href="http://www.cs.utexas.edu/~grauman/courses/spring2007/395T/papers/viola_cvpr2001.pdf">
                     Viola, P. and Jones, M. (2001). Rapid Object Detection using a Boosted Cascade
                     of Simple Features.</a></description></item>
                   <item><description>
                     <a href="http://en.wikipedia.org/wiki/Viola-Jones_object_detection_framework">
                     http://en.wikipedia.org/wiki/Viola-Jones_object_detection_framework</a>
                   </description></item>
                 </list>
               </para>
             </remarks>
             
        </member>
        <member name="M:Accord.Vision.Detection.HaarClassifier.#ctor(Accord.Vision.Detection.HaarCascade)">
            <summary>
              Constructs a new classifier.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarClassifier.#ctor(System.Int32,System.Int32,Accord.Vision.Detection.HaarCascadeStage[])">
            <summary>
              Constructs a new classifier.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarClassifier.Compute(Accord.Imaging.IntegralImage2,System.Drawing.Rectangle)">
            <summary>
              Detects the presence of an object in a given window.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarClassifier.Cascade">
            <summary>
              Gets the cascade of weak-classifiers
              used by this strong classifier.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarClassifier.Scale">
            <summary>
              Gets or sets the scale of the search window
              being currently used by the classifier.
            </summary>
            
        </member>
        <member name="T:Accord.Vision.Detection.HaarCascadeStage">
            <summary>
              Haar Cascade Classifier Stage.
            </summary>
            
            <remarks>
              A Haar Cascade Classifier is composed of several stages. Each stage
              contains a set of classifier trees used in the decision process.
            </remarks>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarCascadeStage.#ctor">
            <summary>
              Constructs a new Haar Cascade Stage.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarCascadeStage.#ctor(System.Double)">
            <summary>
              Constructs a new Haar Cascade Stage.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarCascadeStage.#ctor(System.Double,System.Int32,System.Int32)">
            <summary>
              Constructs a new Haar Cascade Stage.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarCascadeStage.Classify(Accord.Imaging.IntegralImage2,System.Int32,System.Int32,System.Double)">
            <summary>
              Classifies an image as having the searched object or not.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarCascadeStage.Clone">
            <summary>
              Creates a new object that is a copy of the current instance.
            </summary>
            
            <returns>
              A new object that is a copy of this instance.
            </returns>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarCascadeStage.Trees">
            <summary>
              Gets or sets the feature trees and its respective
              feature tree nodes which compose this stage.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarCascadeStage.Threshold">
            <summary>
              Gets or sets the threshold associated with this stage,
              i.e. the minimum value the classifiers should output
              to decide if the image contains the object or not.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarCascadeStage.ParentIndex">
            <summary>
              Gets the index of the parent stage from this stage.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarCascadeStage.NextIndex">
            <summary>
              Gets the index of the next stage from this stage.
            </summary>
            
        </member>
        <member name="T:Accord.Vision.Detection.HaarCascadeSerializationObject">
            <summary>
              Haar Cascade Serialization Root. This class is used
              only for XML serialization/deserialization.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarCascadeSerializationObject.Stages">
            <summary>
              The stages retrieved after deserialization.
            </summary>
        </member>
        <member name="T:Accord.Vision.Detection.HaarFeatureNode">
            <summary>
              Haar Cascade Feature Tree Node.
            </summary>
            
            <remarks>
              The Feature Node is a node belonging to a feature tree,
              containing information about child nodes and an associated 
              <see cref="T:Accord.Vision.Detection.HaarFeature"/>.
            </remarks>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarFeatureNode.#ctor">
            <summary>
              Constructs a new feature tree node.
            </summary>
        </member>
        <member name="M:Accord.Vision.Detection.HaarFeatureNode.#ctor(System.Double,System.Double,System.Double,System.Int32[][])">
            <summary>
              Constructs a new feature tree node.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarFeatureNode.#ctor(System.Double,System.Double,System.Double,System.Boolean,System.Int32[][])">
            <summary>
              Constructs a new feature tree node.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarFeatureNode.Clone">
            <summary>
              Creates a new object that is a copy of the current instance.
            </summary>
            <returns>
              A new object that is a copy of this instance.
            </returns>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarFeatureNode.Threshold">
            <summary>
              Gets the threshold for this feature.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarFeatureNode.LeftValue">
            <summary>
              Gets the left value for this feature.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarFeatureNode.RightValue">
            <summary>
              Gets the right value for this feature.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarFeatureNode.LeftNodeIndex">
            <summary>
              Gets the left node index for this feature.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarFeatureNode.RightNodeIndex">
            <summary>
              Gets the right node index for this feature.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarFeatureNode.Feature">
            <summary>
              Gets the feature associated with this node.
            </summary>
            
        </member>
        <member name="T:Accord.Vision.Detection.HaarFeature">
            <summary>
              Rectangular Haar-like feature container.
            </summary>
            
            <remarks>
              References:
              - http://en.wikipedia.org/wiki/Haar-like_features#Rectangular_Haar-like_features
            </remarks>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarFeature.#ctor">
            <summary>
              Constructs a new Haar-like feature.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarFeature.#ctor(Accord.Vision.Detection.HaarRectangle[])">
            <summary>
              Constructs a new Haar-like feature.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarFeature.#ctor(System.Int32[][])">
            <summary>
              Constructs a new Haar-like feature.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarFeature.#ctor(System.Boolean,System.Int32[][])">
            <summary>
              Constructs a new Haar-like feature.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarFeature.GetSum(Accord.Imaging.IntegralImage2,System.Int32,System.Int32)">
            <summary>
              Gets the sum of the areas of the rectangular features in an integral image.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarFeature.SetScaleAndWeight(System.Single,System.Single)">
            <summary>
              Sets the scale and weight of a Haar-like rectangular feature container.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarFeature.Clone">
            <summary>
              Creates a new object that is a copy of the current instance.
            </summary>
            
            <returns>
              A new object that is a copy of this instance.
            </returns>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarFeature.Tilted">
            <summary>
              Gets or sets whether this feature is tilted.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarFeature.Rectangles">
            <summary>
              Gets or sets the Haar rectangles for this feature.
            </summary>
            
        </member>
        <member name="T:Accord.Vision.Detection.HaarRectangle">
            <summary>
              Scalable rectangular area.
            </summary>
            
            <remarks>
              A rectangle which can be at any position and scale within the original image.
            </remarks>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarRectangle.#ctor(System.Int32[])">
            <summary>
              Constructs a new Haar-like feature rectangle.
            </summary>
            <param name="values">Values for this rectangle.</param>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarRectangle.#ctor(System.Int32,System.Int32,System.Int32,System.Int32,System.Single)">
            <summary>
              Constructs a new Haar-like feature rectangle.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarRectangle.ScaleRectangle(System.Single)">
            <summary>
              Scales the values of this rectangle.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarRectangle.ScaleWeight(System.Single)">
            <summary>
              Scales the weight of this rectangle.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarRectangle.Parse(System.String)">
            <summary>
              Converts from a string representation.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarRectangle.Clone">
            <summary>
              Creates a new object that is a copy of the current instance.
            </summary>
            
            <returns>
              A new object that is a copy of this instance.
            </returns>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarRectangle.X">
            <summary>
              Gets or sets the x-coordinate of this Haar feature rectangle.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarRectangle.Y">
            <summary>
              Gets or sets the y-coordinate of this Haar feature rectangle.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarRectangle.Width">
            <summary>
              Gets or sets the width of this Haar feature rectangle.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarRectangle.Height">
            <summary>
              Gets or sets the height of this Haar feature rectangle.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarRectangle.Weight">
            <summary>
              Gets or sets the weight of this Haar feature rectangle.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarRectangle.ScaledX">
            <summary>
              Gets or sets the scaled x-coordinate of this Haar feature rectangle.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarRectangle.ScaledY">
            <summary>
              Gets or sets the scaled y-coordinate of this Haar feature rectangle.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarRectangle.ScaledWidth">
            <summary>
              Gets or sets the scaled width of this Haar feature rectangle.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarRectangle.ScaledHeight">
            <summary>
              Gets or sets the scaled height of this Haar feature rectangle.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarRectangle.ScaledWeight">
            <summary>
              Gets or sets the scaled weight of this Haar feature rectangle.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarRectangle.Area">
            <summary>
              Gets the area of this rectangle.
            </summary>
            
        </member>
        <member name="T:Accord.Vision.Detection.IObjectDetector">
            <summary>
              Object detector interface.
            </summary>
        </member>
        <member name="M:Accord.Vision.Detection.IObjectDetector.ProcessFrame(AForge.Imaging.UnmanagedImage)">
            <summary>
              Process a new image scene looking for objects.
            </summary>
        </member>
        <member name="P:Accord.Vision.Detection.IObjectDetector.DetectedObjects">
            <summary>
              Gets the location of the detected objects.
            </summary>
        </member>
        <member name="T:Accord.Vision.Detection.ObjectDetectorSearchMode">
            <summary>
              Object detector options for the search procedure.
            </summary>
            
        </member>
        <member name="F:Accord.Vision.Detection.ObjectDetectorSearchMode.Default">
            <summary>
              Entire image will be scanned.
            </summary>
            
        </member>
        <member name="F:Accord.Vision.Detection.ObjectDetectorSearchMode.Single">
            <summary>
              Only a single object will be retrieved.
            </summary>
            
        </member>
        <member name="F:Accord.Vision.Detection.ObjectDetectorSearchMode.NoOverlap">
            <summary>
              If a object has already been detected inside an area,
              it will not be scanned twice for inner or overlapping
              objects, saving computation time.
            </summary>
            
        </member>
        <member name="F:Accord.Vision.Detection.ObjectDetectorSearchMode.Average">
            <summary>
              If several objects are located within one another, 
              they will be averaged. Additionally, objects which
              have not been detected sufficient times may be dropped
              by setting <see cref="P:Accord.Vision.Detection.HaarObjectDetector.Suppression"/>.
            </summary>
            
        </member>
        <member name="T:Accord.Vision.Detection.ObjectDetectorScalingMode">
            <summary>
              Object detector options for window scaling.
            </summary>
            
        </member>
        <member name="F:Accord.Vision.Detection.ObjectDetectorScalingMode.GreaterToSmaller">
            <summary>
              Will start with a big search window and
              gradually scale into smaller ones.
            </summary>
            
        </member>
        <member name="F:Accord.Vision.Detection.ObjectDetectorScalingMode.SmallerToGreater">
            <summary>
              Will start with small search windows and
              gradually scale into greater ones.
            </summary>
            
        </member>
        <member name="T:Accord.Vision.Detection.HaarObjectDetector">
            <summary>
              Viola-Jones Object Detector based on Haar-like features.
            </summary>
            <remarks>
            
            <para>
              The Viola-Jones object detection framework is the first object detection framework
              to provide competitive object detection rates in real-time proposed in 2001 by Paul
              Viola and Michael Jones. Although it can be trained to detect a variety of object
              classes, it was motivated primarily by the problem of face detection.</para>
              
            <para>
              The implementation of this code has used Viola and Jones' original publication, the
              OpenCV Library and the Marilena Project as reference. OpenCV is released under a BSD
              license, it is free for both academic and commercial use. Please be aware that some
              particular versions of the Haar object detection framework are patented by Viola and
              Jones and may be subject to restrictions for use in commercial applications. The code
              has been implemented with full support for tilted Haar features from the ground up.</para>
              
             <para>
                References:
                <list type="bullet">
                  <item><description>
                    <a href="http://www.cs.utexas.edu/~grauman/courses/spring2007/395T/papers/viola_cvpr2001.pdf">
                    Viola, P. and Jones, M. (2001). Rapid Object Detection using a Boosted Cascade
                    of Simple Features.</a></description></item>
                  <item><description>
                    <a href="http://en.wikipedia.org/wiki/Viola-Jones_object_detection_framework">
                    http://en.wikipedia.org/wiki/Viola-Jones_object_detection_framework</a>
                  </description></item>
                </list>
              </para>
            </remarks>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarObjectDetector.#ctor(Accord.Vision.Detection.HaarCascade)">
            <summary>
              Constructs a new Haar object detector.
            </summary>
            
            <param name="cascade">
              The <see cref="T:Accord.Vision.Detection.HaarCascade"/> to use in the detector's classifier.
              For the default face cascade, please take a look on
              <see cref="T:Accord.Vision.Detection.Cascades.FaceHaarCascade"/>.
            </param>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarObjectDetector.#ctor(Accord.Vision.Detection.HaarCascade,System.Int32)">
            <summary>
              Constructs a new Haar object detector.
            </summary>
            
            <param name="cascade">
              The <see cref="T:Accord.Vision.Detection.HaarCascade"/> to use in the detector's classifier.
              For the default face cascade, please take a look on
              <see cref="T:Accord.Vision.Detection.Cascades.FaceHaarCascade"/>.</param>
            <param name="minSize">
              Minimum window size to consider when searching for 
              objects. Default value is <c>15</c>.</param>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarObjectDetector.#ctor(Accord.Vision.Detection.HaarCascade,System.Int32,Accord.Vision.Detection.ObjectDetectorSearchMode)">
            <summary>
              Constructs a new Haar object detector.
            </summary>
            
            <param name="cascade">
              The <see cref="T:Accord.Vision.Detection.HaarCascade"/> to use in the detector's classifier.
              For the default face cascade, please take a look on
              <see cref="T:Accord.Vision.Detection.Cascades.FaceHaarCascade"/>.
            </param>
            <param name="minSize">
              Minimum window size to consider when searching for
              objects. Default value is <c>15</c>.</param>
            <param name="searchMode">The <see cref="T:Accord.Vision.Detection.ObjectDetectorSearchMode"/> to use
              during search. Please see documentation of <see cref="T:Accord.Vision.Detection.ObjectDetectorSearchMode"/>
              for details. Default value is <see cref="F:Accord.Vision.Detection.ObjectDetectorSearchMode.NoOverlap"/></param>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarObjectDetector.#ctor(Accord.Vision.Detection.HaarCascade,System.Int32,Accord.Vision.Detection.ObjectDetectorSearchMode,System.Single)">
            <summary>
              Constructs a new Haar object detector.
            </summary>
            
            <param name="cascade">
              The <see cref="T:Accord.Vision.Detection.HaarCascade"/> to use in the detector's classifier.
              For the default face cascade, please take a look on
              <see cref="T:Accord.Vision.Detection.Cascades.FaceHaarCascade"/>.</param>
            <param name="minSize">
              Minimum window size to consider when searching for
              objects. Default value is <c>15</c>.</param>
            <param name="searchMode">
              The <see cref="T:Accord.Vision.Detection.ObjectDetectorSearchMode"/> to use
              during search. Please see documentation of <see cref="T:Accord.Vision.Detection.ObjectDetectorSearchMode"/>
              for details. Default value is <see cref="F:Accord.Vision.Detection.ObjectDetectorSearchMode.NoOverlap"/></param>
            <param name="scaleFactor">The re-scaling factor to use when re-scaling the window during search.</param>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarObjectDetector.#ctor(Accord.Vision.Detection.HaarCascade,System.Int32,Accord.Vision.Detection.ObjectDetectorSearchMode,System.Single,Accord.Vision.Detection.ObjectDetectorScalingMode)">
            <summary>
              Constructs a new Haar object detector.
            </summary>
            
            <param name="cascade">
              The <see cref="T:Accord.Vision.Detection.HaarCascade"/> to use in the detector's classifier.
              For the default face cascade, please take a look on
              <see cref="T:Accord.Vision.Detection.Cascades.FaceHaarCascade"/>. </param>
            <param name="minSize">
              Minimum window size to consider when searching for
              objects. Default value is <c>15</c>.</param>
            <param name="searchMode">The <see cref="T:Accord.Vision.Detection.ObjectDetectorSearchMode"/> to use
              during search. Please see documentation of <see cref="T:Accord.Vision.Detection.ObjectDetectorSearchMode"/>
              for details. Default is <see cref="F:Accord.Vision.Detection.ObjectDetectorSearchMode.NoOverlap"/>.</param>
            <param name="scaleFactor">The scaling factor to rescale the window
              during search. Default value is <c>1.2f</c>.</param>
            <param name="scalingMode">The <see cref="T:Accord.Vision.Detection.ObjectDetectorScalingMode"/> to use
              when re-scaling the search window during search. Default is
              <see cref="F:Accord.Vision.Detection.ObjectDetectorScalingMode.SmallerToGreater"/>.</param>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarObjectDetector.ProcessFrame(System.Drawing.Bitmap)">
            <summary>
              Performs object detection on the given frame.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Detection.HaarObjectDetector.ProcessFrame(AForge.Imaging.UnmanagedImage)">
            <summary>
              Performs object detection on the given frame.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarObjectDetector.UseParallelProcessing">
            <summary>
              Gets or sets a value indicating whether this <see cref="T:Accord.Vision.Detection.HaarObjectDetector"/>
              should scan the image using multiple threads. This setting can only be changed
              to true on .NET version which support the Parallel Tasks framework (4.0+).
            </summary>
            
            <value><c>true</c> to use multiple threads; otherwise, <c>false</c>.</value>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarObjectDetector.MinSize">
            <summary>
              Minimum window size to consider when searching objects.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarObjectDetector.MaxSize">
            <summary>
              Maximum window size to consider when searching objects.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarObjectDetector.Channel">
            <summary>
              Gets or sets the color channel to use when processing color images. 
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarObjectDetector.ScalingFactor">
            <summary>
              Gets or sets the scaling factor to rescale the window during search.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarObjectDetector.SearchMode">
            <summary>
              Gets or sets the desired searching method.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarObjectDetector.ScalingMode">
            <summary>
              Gets or sets the desired scaling method.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarObjectDetector.Suppression">
            <summary>
              Gets or sets the minimum threshold used to suppress rectangles which
              have not been detected sufficient number of times. This property only
              has effect when <see cref="P:Accord.Vision.Detection.HaarObjectDetector.SearchMode"/> is set to <see cref="F:Accord.Vision.Detection.ObjectDetectorSearchMode.Average"/>.
            </summary>
            
            <remarks>
            <para>
              The value of this property represents the minimum amount of detections
              made inside a region to report this region as an actual detection. For
              example, setting this property to two will discard all regions which 
              had not achieved at least two detected rectangles within it.</para>
              
            <para>
              Setting this property to a value higher than zero may decrease the
              number of false positives.</para>
            </remarks>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarObjectDetector.DetectedObjects">
            <summary>
              Gets the detected objects bounding boxes.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarObjectDetector.Classifier">
            <summary>
              Gets the internal Cascade Classifier used by this detector.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Detection.HaarObjectDetector.Steady">
            <summary>
              Gets how many frames the object has
              been detected in a steady position.
            </summary>
            <value>
              The number of frames the detected object
              has been in a steady position.</value>
              
        </member>
        <member name="T:AForge.Vision.Motion.BlobCountingObjectsProcessing">
            <summary>
            Motion processing algorithm, which counts separate moving objects and highlights them.
            </summary>
            
            <remarks><para>The aim of this motion processing algorithm is to count separate objects
            in the motion frame, which is provided by <see cref="T:AForge.Vision.Motion.IMotionDetector">motion detection algorithm</see>.
            In the case if <see cref="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.HighlightMotionRegions"/> property is set to <see langword="true"/>,
            found objects are also highlighted on the original video frame. The algorithm
            counts and highlights only those objects, which size satisfies <see cref="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.MinObjectsWidth"/>
            and <see cref="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.MinObjectsHeight"/> properties.</para>
            
            <para><note>The motion processing algorithm is supposed to be used only with motion detection
            algorithms, which are based on finding difference with background frame
            (see <see cref="T:AForge.Vision.Motion.SimpleBackgroundModelingDetector"/> and <see cref="T:AForge.Vision.Motion.CustomFrameDifferenceDetector"/>
            as simple implementations) and allow extract moving objects clearly.</note></para>
            
            <para>Sample usage:</para>
            <code>
            // create instance of motion detection algorithm
            IMotionDetector motionDetector = new ... ;
            // create instance of motion processing algorithm
            BlobCountingObjectsProcessing motionProcessing = new BlobCountingObjectsProcessing( );
            // create motion detector
            MotionDetector detector = new MotionDetector( motionDetector, motionProcessing );
            
            // continuously feed video frames to motion detector
            while ( ... )
            {
                // process new video frame and check motion level
                if ( detector.ProcessFrame( videoFrame ) &gt; 0.02 )
                {
                    // check number of detected objects
                    if ( motionProcessing.ObjectsCount &gt; 1 )
                    {
                        // ...
                    }
                }
            }
            </code>
            </remarks>
            
            <seealso cref="T:AForge.Vision.Motion.MotionDetector"/>
            <seealso cref="T:AForge.Vision.Motion.IMotionDetector"/>
            
        </member>
        <member name="T:AForge.Vision.Motion.IMotionProcessing">
             <summary>
             Interface of motion processing algorithm.
             </summary>
            
             <remarks><para>The interface specifies methods, which should be implemented
             by all motion processng algorithms - algorithm which perform further post processing
             of detected motion, which is done by motion detection algorithms (see <see cref="T:AForge.Vision.Motion.IMotionDetector"/>).
             </para></remarks>
             
             <seealso cref="T:AForge.Vision.Motion.MotionDetector"/>
             <seealso cref="T:AForge.Vision.Motion.IMotionDetector"/>
            
        </member>
        <member name="M:AForge.Vision.Motion.IMotionProcessing.ProcessFrame(AForge.Imaging.UnmanagedImage,AForge.Imaging.UnmanagedImage)">
            <summary>
            Process video and motion frames doing further post processing after
            performed motion detection.
            </summary>
            
            <param name="videoFrame">Original video frame.</param>
            <param name="motionFrame">Motion frame provided by motion detection
            algorithm (see <see cref="T:AForge.Vision.Motion.IMotionDetector"/>).</param>
            
            <remarks><para>The method does father post processing of detected motion.
            Type of motion post processing is specified by specific implementation
            of the <see cref="T:AForge.Vision.Motion.IMotionProcessing"/> interface - it may motion
            area highlighting, motion objects counting, etc.</para></remarks>
            
        </member>
        <member name="M:AForge.Vision.Motion.IMotionProcessing.Reset">
             <summary>
             Reset internal state of motion processing algorithm.
             </summary>
             
             <remarks><para>The method allows to reset internal state of motion processing
             algorithm and prepare it for processing of next video stream or to restart
             the algorithm.</para>
             
             <para><note>Some motion processing algorithms may not have any stored internal
             states and may just process provided video frames without relying on any motion
             history etc. In this case such algorithms provide empty implementation of this method.</note></para>
             </remarks>
            
        </member>
        <member name="M:AForge.Vision.Motion.BlobCountingObjectsProcessing.#ctor">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.BlobCountingObjectsProcessing"/> class.
            </summary>
            
        </member>
        <member name="M:AForge.Vision.Motion.BlobCountingObjectsProcessing.#ctor(System.Boolean)">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.BlobCountingObjectsProcessing"/> class.
            </summary>
            
            <param name="highlightMotionRegions">Highlight motion regions or not (see <see cref="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.HighlightMotionRegions"/> property).</param>
            
        </member>
        <member name="M:AForge.Vision.Motion.BlobCountingObjectsProcessing.#ctor(System.Int32,System.Int32)">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.BlobCountingObjectsProcessing"/> class.
            </summary>
            
            <param name="minWidth">Minimum width of acceptable object (see <see cref="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.MinObjectsWidth"/> property).</param>
            <param name="minHeight">Minimum height of acceptable object (see <see cref="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.MinObjectsHeight"/> property).</param>
            
        </member>
        <member name="M:AForge.Vision.Motion.BlobCountingObjectsProcessing.#ctor(System.Int32,System.Int32,System.Drawing.Color)">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.BlobCountingObjectsProcessing"/> class.
            </summary>
            
            <param name="minWidth">Minimum width of acceptable object (see <see cref="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.MinObjectsWidth"/> property).</param>
            <param name="minHeight">Minimum height of acceptable object (see <see cref="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.MinObjectsHeight"/> property).</param>
            <param name="highlightColor">Color used to highlight motion regions.</param>
            
        </member>
        <member name="M:AForge.Vision.Motion.BlobCountingObjectsProcessing.#ctor(System.Int32,System.Int32,System.Boolean)">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.BlobCountingObjectsProcessing"/> class.
            </summary>
            
            <param name="minWidth">Minimum width of acceptable object (see <see cref="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.MinObjectsWidth"/> property).</param>
            <param name="minHeight">Minimum height of acceptable object (see <see cref="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.MinObjectsHeight"/> property).</param>
            <param name="highlightMotionRegions">Highlight motion regions or not (see <see cref="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.HighlightMotionRegions"/> property).</param>
            
        </member>
        <member name="M:AForge.Vision.Motion.BlobCountingObjectsProcessing.ProcessFrame(AForge.Imaging.UnmanagedImage,AForge.Imaging.UnmanagedImage)">
            <summary>
            Process video and motion frames doing further post processing after
            performed motion detection.
            </summary>
            
            <param name="videoFrame">Original video frame.</param>
            <param name="motionFrame">Motion frame provided by motion detection
            algorithm (see <see cref="T:AForge.Vision.Motion.IMotionDetector"/>).</param>
            
            <remarks><para>Processes provided motion frame and counts number of separate
            objects, which size satisfies <see cref="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.MinObjectsWidth"/> and <see cref="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.MinObjectsHeight"/>
            properties. In the case if <see cref="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.HighlightMotionRegions"/> property is
            set to <see langword="true"/>, the found object are also highlighted on the
            original video frame.
            </para></remarks>
            
            <exception cref="T:AForge.Imaging.InvalidImagePropertiesException">Motion frame is not 8 bpp image, but it must be so.</exception>
            <exception cref="T:AForge.Imaging.UnsupportedImageFormatException">Video frame must be 8 bpp grayscale image or 24/32 bpp color image.</exception>
            
        </member>
        <member name="M:AForge.Vision.Motion.BlobCountingObjectsProcessing.Reset">
             <summary>
             Reset internal state of motion processing algorithm.
             </summary>
             
             <remarks><para>The method allows to reset internal state of motion processing
             algorithm and prepare it for processing of next video stream or to restart
             the algorithm.</para></remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.HighlightMotionRegions">
             <summary>
             Highlight motion regions or not.
             </summary>
             
             <remarks><para>The property specifies if detected moving objects should be highlighted
             with rectangle or not.</para>
             
             <para>Default value is set to <see langword="true"/>.</para>
            
             <para><note>Turning the value on leads to additional processing time of video frame.</note></para>
             </remarks>
             
        </member>
        <member name="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.HighlightColor">
            <summary>
            Color used to highlight motion regions.
            </summary>
            
            <remarks>
            <para>Default value is set to <b>red</b> color.</para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.MinObjectsWidth">
            <summary>
            Minimum width of acceptable object.
            </summary>
            
            <remarks><para>The property sets minimum width of an object to count and highlight. If
            objects have smaller width, they are not counted and are not highlighted.</para>
            
            <para>Default value is set to <b>10</b>.</para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.MinObjectsHeight">
            <summary>
            Minimum height of acceptable object.
            </summary>
            
            <remarks><para>The property sets minimum height of an object to count and highlight. If
            objects have smaller height, they are not counted and are not highlighted.</para>
            
            <para>Default value is set to <b>10</b>.</para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.ObjectsCount">
            <summary>
            Number of detected objects.
            </summary>
            
            <remarks><para>The property provides number of moving objects detected by
            the last call of <see cref="M:AForge.Vision.Motion.BlobCountingObjectsProcessing.ProcessFrame(AForge.Imaging.UnmanagedImage,AForge.Imaging.UnmanagedImage)"/> method.</para></remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.BlobCountingObjectsProcessing.ObjectRectangles">
            <summary>
            Rectangles of moving objects.
            </summary>
            
            <remarks><para>The property provides array of moving objects' rectangles
            detected by the last call of <see cref="M:AForge.Vision.Motion.BlobCountingObjectsProcessing.ProcessFrame(AForge.Imaging.UnmanagedImage,AForge.Imaging.UnmanagedImage)"/> method.</para></remarks>
            
        </member>
        <member name="T:AForge.Vision.Motion.CustomFrameDifferenceDetector">
            <summary>
            Motion detector based on difference with predefined background frame.
            </summary>
            
            <remarks><para>The class implements motion detection algorithm, which is based on
            difference of current video frame with predefined background frame. The <see cref="P:AForge.Vision.Motion.CustomFrameDifferenceDetector.MotionFrame">difference frame</see>
            is thresholded and the <see cref="P:AForge.Vision.Motion.CustomFrameDifferenceDetector.MotionLevel">amount of difference pixels</see> is calculated.
            To suppress stand-alone noisy pixels erosion morphological operator may be applied, which
            is controlled by <see cref="P:AForge.Vision.Motion.CustomFrameDifferenceDetector.SuppressNoise"/> property.</para>
            
            <para><note>In the case if precise motion area's borders are required (for example,
            for further motion post processing), then <see cref="P:AForge.Vision.Motion.CustomFrameDifferenceDetector.KeepObjectsEdges"/> property
            may be used to restore borders after noise suppression.</note></para>
            
            <para><note>In the case if custom background frame is not specified by using
            <see cref="M:AForge.Vision.Motion.CustomFrameDifferenceDetector.SetBackgroundFrame(System.Drawing.Bitmap)"/> method, the algorithm takes first video frame
            as a background frame and calculates difference of further video frames with it.</note></para>
            
            <para>Unlike <see cref="T:AForge.Vision.Motion.TwoFramesDifferenceDetector"/> motion detection algorithm, this algorithm
            allows to identify quite clearly all objects, which are not part of the background (scene) -
            most likely moving objects.</para>
            
            <para>Sample usage:</para>
            <code>
            // create motion detector
            MotionDetector detector = new MotionDetector(
                new CustomFrameDifferenceDetector( ),
                new MotionAreaHighlighting( ) );
            
            // continuously feed video frames to motion detector
            while ( ... )
            {
                // process new video frame and check motion level
                if ( detector.ProcessFrame( videoFrame ) &gt; 0.02 )
                {
                    // ring alarm or do somethng else
                }
            }
            </code>
            </remarks>
            
            <seealso cref="T:AForge.Vision.Motion.MotionDetector"/>
            
        </member>
        <member name="T:AForge.Vision.Motion.IMotionDetector">
             <summary>
             Interface of motion detector algorithm.
             </summary>
             
             <remarks><para>The interface specifies methods, which should be implemented
             by all motion detection algorithms - algorithms which perform processing of video
             frames in order to detect motion. Amount of detected motion may be checked using
             <see cref="P:AForge.Vision.Motion.IMotionDetector.MotionLevel"/> property. Also <see cref="P:AForge.Vision.Motion.IMotionDetector.MotionFrame"/> property may
             be used in order to see all the detected motion areas. For example, the <see cref="P:AForge.Vision.Motion.IMotionDetector.MotionFrame"/> property
             is used by motion processing algorithms for further motion post processing, like
             highlighting motion areas, counting number of detected moving object, etc.
             </para></remarks>
             
             <seealso cref="T:AForge.Vision.Motion.MotionDetector"/>
             <seealso cref="T:AForge.Vision.Motion.IMotionProcessing"/>
            
            
        </member>
        <member name="M:AForge.Vision.Motion.IMotionDetector.ProcessFrame(AForge.Imaging.UnmanagedImage)">
            <summary>
            Process new video frame.
            </summary>
            
            <param name="videoFrame">Video frame to process (detect motion in).</param>
            
            <remarks><para>Processes new frame from video source and detects motion in it.</para></remarks>
            
        </member>
        <member name="M:AForge.Vision.Motion.IMotionDetector.Reset">
            <summary>
            Reset motion detector to initial state.
            </summary>
            
            <remarks><para>Resets internal state and variables of motion detection algorithm.
            Usually this is required to be done before processing new video source, but
            may be also done at any time to restart motion detection algorithm.</para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.IMotionDetector.MotionLevel">
            <summary>
            Motion level value, [0, 1].
            </summary>
            
            <remarks><para>Amount of changes in the last processed frame. For example, if value of
            this property equals to 0.1, then it means that last processed frame has 10% of changes
            (however it is up to specific implementation to decide how to compare specified frame).</para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.IMotionDetector.MotionFrame">
            <summary>
            Motion frame containing detected areas of motion.
            </summary>
            
            <remarks><para>Motion frame is a grayscale image, which shows areas of detected motion.
            All black pixels in the motion frame correspond to areas, where no motion is
            detected. But white pixels correspond to areas, where motion is detected.</para></remarks>
            
        </member>
        <member name="M:AForge.Vision.Motion.CustomFrameDifferenceDetector.#ctor">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.CustomFrameDifferenceDetector"/> class.
            </summary>
        </member>
        <member name="M:AForge.Vision.Motion.CustomFrameDifferenceDetector.#ctor(System.Boolean)">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.CustomFrameDifferenceDetector"/> class.
            </summary>
            
            <param name="suppressNoise">Suppress noise in video frames or not (see <see cref="P:AForge.Vision.Motion.CustomFrameDifferenceDetector.SuppressNoise"/> property).</param>
            
        </member>
        <member name="M:AForge.Vision.Motion.CustomFrameDifferenceDetector.#ctor(System.Boolean,System.Boolean)">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.CustomFrameDifferenceDetector"/> class.
            </summary>
            
            <param name="suppressNoise">Suppress noise in video frames or not (see <see cref="P:AForge.Vision.Motion.CustomFrameDifferenceDetector.SuppressNoise"/> property).</param>
            <param name="keepObjectEdges">Restore objects edges after noise suppression or not (see <see cref="P:AForge.Vision.Motion.CustomFrameDifferenceDetector.KeepObjectsEdges"/> property).</param>
            
        </member>
        <member name="M:AForge.Vision.Motion.CustomFrameDifferenceDetector.ProcessFrame(AForge.Imaging.UnmanagedImage)">
            <summary>
            Process new video frame.
            </summary>
            
            <param name="videoFrame">Video frame to process (detect motion in).</param>
            
            <remarks><para>Processes new frame from video source and detects motion in it.</para>
            
            <para>Check <see cref="P:AForge.Vision.Motion.CustomFrameDifferenceDetector.MotionLevel"/> property to get information about amount of motion
            (changes) in the processed frame.</para>
            </remarks>
            
        </member>
        <member name="M:AForge.Vision.Motion.CustomFrameDifferenceDetector.Reset">
            <summary>
            Reset motion detector to initial state.
            </summary>
            
            <remarks><para>Resets internal state and variables of motion detection algorithm.
            Usually this is required to be done before processing new video source, but
            may be also done at any time to restart motion detection algorithm.</para>
            
            <para><note>In the case if custom background frame was set using
            <see cref="M:AForge.Vision.Motion.CustomFrameDifferenceDetector.SetBackgroundFrame(System.Drawing.Bitmap)"/> method, this method does not reset it.
            The method resets only automatically generated background frame.
            </note></para>
            </remarks>
            
        </member>
        <member name="M:AForge.Vision.Motion.CustomFrameDifferenceDetector.SetBackgroundFrame(System.Drawing.Bitmap)">
            <summary>
            Set background frame.
            </summary>
            
            <param name="backgroundFrame">Background frame to set.</param>
            
            <remarks><para>The method sets background frame, which will be used to calculate
            difference with.</para></remarks>
            
        </member>
        <member name="M:AForge.Vision.Motion.CustomFrameDifferenceDetector.SetBackgroundFrame(System.Drawing.Imaging.BitmapData)">
            <summary>
            Set background frame.
            </summary>
            
            <param name="backgroundFrame">Background frame to set.</param>
            
            <remarks><para>The method sets background frame, which will be used to calculate
            difference with.</para></remarks>
            
        </member>
        <member name="M:AForge.Vision.Motion.CustomFrameDifferenceDetector.SetBackgroundFrame(AForge.Imaging.UnmanagedImage)">
            <summary>
            Set background frame.
            </summary>
            
            <param name="backgroundFrame">Background frame to set.</param>
            
            <remarks><para>The method sets background frame, which will be used to calculate
            difference with.</para></remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.CustomFrameDifferenceDetector.DifferenceThreshold">
            <summary>
            Difference threshold value, [1, 255].
            </summary>
            
            <remarks><para>The value specifies the amount off difference between pixels, which is treated
            as motion pixel.</para>
            
            <para>Default value is set to <b>15</b>.</para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.CustomFrameDifferenceDetector.MotionLevel">
            <summary>
            Motion level value, [0, 1].
            </summary>
            
            <remarks><para>Amount of changes in the last processed frame. For example, if value of
            this property equals to 0.1, then it means that last processed frame has 10% difference
            with defined background frame.</para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.CustomFrameDifferenceDetector.MotionFrame">
             <summary>
             Motion frame containing detected areas of motion.
             </summary>
             
             <remarks><para>Motion frame is a grayscale image, which shows areas of detected motion.
             All black pixels in the motion frame correspond to areas, where no motion is
             detected. But white pixels correspond to areas, where motion is detected.</para>
             
             <para><note>The property is set to <see langword="null"/> after processing of the first
             video frame by the algorithm in the case if custom background frame was not set manually
             by using <see cref="M:AForge.Vision.Motion.CustomFrameDifferenceDetector.SetBackgroundFrame(System.Drawing.Bitmap)"/> method (it will be not <see langword="null"/>
             after second call in this case). If correct custom background
             was set then the property should bet set to estimated motion frame after
             <see cref="M:AForge.Vision.Motion.CustomFrameDifferenceDetector.ProcessFrame(AForge.Imaging.UnmanagedImage)"/> method call.</note></para>
             </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.CustomFrameDifferenceDetector.SuppressNoise">
            <summary>
            Suppress noise in video frames or not.
            </summary>
            
            <remarks><para>The value specifies if additional filtering should be
            done to suppress standalone noisy pixels by applying 3x3 erosion image processing
            filter. See <see cref="P:AForge.Vision.Motion.CustomFrameDifferenceDetector.KeepObjectsEdges"/> property, if it is required to restore
            edges of objects, which are not noise.</para>
            
            <para>Default value is set to <see langword="true"/>.</para>
            
            <para><note>Turning the value on leads to more processing time of video frame.</note></para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.CustomFrameDifferenceDetector.KeepObjectsEdges">
            <summary>
            Restore objects edges after noise suppression or not.
            </summary>
            
            <remarks><para>The value specifies if additional filtering should be done
            to restore objects' edges after noise suppression by applying 3x3 dilatation
            image processing filter.</para>
            
            <para>Default value is set to <see langword="false"/>.</para>
            
            <para><note>Turning the value on leads to more processing time of video frame.</note></para>
            </remarks>
            
        </member>
        <member name="T:AForge.Vision.Motion.GridMotionAreaProcessing">
            <summary>
            Motion processing algorithm, which performs grid processing of motion frame.
            </summary>
            
            <remarks><para>The aim of this motion processing algorithm is to do grid processing
            of motion frame. This means that entire motion frame is divided by a grid into
            certain amount of cells and the motion level is calculated for each cell. The
            information about each cell's motion level may be retrieved using <see cref="P:AForge.Vision.Motion.GridMotionAreaProcessing.MotionGrid"/>
            property.</para>
            
            <para><para>In addition the algorithm can highlight those cells, which have motion
            level above the specified threshold (see <see cref="P:AForge.Vision.Motion.GridMotionAreaProcessing.MotionAmountToHighlight"/>
            property). To enable this it is required to set <see cref="P:AForge.Vision.Motion.GridMotionAreaProcessing.HighlightMotionGrid"/>
            property to <see langword="true"/>.</para></para>
            
            <para>Sample usage:</para>
            <code>
            // create instance of motion detection algorithm
            IMotionDetector motionDetector = new ... ;
            // create instance of motion processing algorithm
            GridMotionAreaProcessing motionProcessing = new GridMotionAreaProcessing( 16, 16 );
            // create motion detector
            MotionDetector detector = new MotionDetector( motionDetector, motionProcessing );
            
            // continuously feed video frames to motion detector
            while ( ... )
            {
                // process new video frame
                detector.ProcessFrame( videoFrame );
                
                // check motion level in 5th row 8th column
                if ( motionProcessing.MotionGrid[5, 8] &gt; 0.15 )
                {
                    // ...
                }
            }
            </code>
            </remarks>
            
            <seealso cref="T:AForge.Vision.Motion.MotionDetector"/>
            <seealso cref="T:AForge.Vision.Motion.IMotionDetector"/>
            
        </member>
        <member name="M:AForge.Vision.Motion.GridMotionAreaProcessing.#ctor">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.GridMotionAreaProcessing"/> class.
            </summary>
            
        </member>
        <member name="M:AForge.Vision.Motion.GridMotionAreaProcessing.#ctor(System.Int32,System.Int32)">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.GridMotionAreaProcessing"/> class.
            </summary>
            
            <param name="gridWidth">Width of motion grid (see <see cref="P:AForge.Vision.Motion.GridMotionAreaProcessing.GridWidth"/> property).</param>
            <param name="gridHeight">Height of motion grid (see <see cref="P:AForge.Vision.Motion.GridMotionAreaProcessing.GridHeight"/> property).</param>
            
        </member>
        <member name="M:AForge.Vision.Motion.GridMotionAreaProcessing.#ctor(System.Int32,System.Int32,System.Boolean)">
             <summary>
             Initializes a new instance of the <see cref="T:AForge.Vision.Motion.GridMotionAreaProcessing"/> class.
             </summary>
             
             <param name="gridWidth">Width of motion grid (see <see cref="P:AForge.Vision.Motion.GridMotionAreaProcessing.GridWidth"/> property).</param>
             <param name="gridHeight">Height of motion grid (see <see cref="P:AForge.Vision.Motion.GridMotionAreaProcessing.GridHeight"/> property).</param>
             <param name="highlightMotionGrid">Highlight motion regions or not (see <see cref="P:AForge.Vision.Motion.GridMotionAreaProcessing.HighlightMotionGrid"/> property).</param>
            
        </member>
        <member name="M:AForge.Vision.Motion.GridMotionAreaProcessing.#ctor(System.Int32,System.Int32,System.Boolean,System.Single)">
             <summary>
             Initializes a new instance of the <see cref="T:AForge.Vision.Motion.GridMotionAreaProcessing"/> class.
             </summary>
             
             <param name="gridWidth">Width of motion grid (see <see cref="P:AForge.Vision.Motion.GridMotionAreaProcessing.GridWidth"/> property).</param>
             <param name="gridHeight">Height of motion grid (see <see cref="P:AForge.Vision.Motion.GridMotionAreaProcessing.GridHeight"/> property).</param>
             <param name="highlightMotionGrid">Highlight motion regions or not (see <see cref="P:AForge.Vision.Motion.GridMotionAreaProcessing.HighlightMotionGrid"/> property).</param>
             <param name="motionAmountToHighlight">Motion amount to highlight cell (see <see cref="P:AForge.Vision.Motion.GridMotionAreaProcessing.MotionAmountToHighlight"/> property).</param>
            
        </member>
        <member name="M:AForge.Vision.Motion.GridMotionAreaProcessing.ProcessFrame(AForge.Imaging.UnmanagedImage,AForge.Imaging.UnmanagedImage)">
             <summary>
             Process video and motion frames doing further post processing after
             performed motion detection.
             </summary>
             
             <param name="videoFrame">Original video frame.</param>
             <param name="motionFrame">Motion frame provided by motion detection
             algorithm (see <see cref="T:AForge.Vision.Motion.IMotionDetector"/>).</param>
             
             <remarks><para>Processes provided motion frame and calculates motion level
             for each grid's cell. In the case if <see cref="P:AForge.Vision.Motion.GridMotionAreaProcessing.HighlightMotionGrid"/> property is
             set to <see langword="true"/>, the cell with motion level above threshold are
             highlighted.</para></remarks>
            
             <exception cref="T:AForge.Imaging.InvalidImagePropertiesException">Motion frame is not 8 bpp image, but it must be so.</exception>
             <exception cref="T:AForge.Imaging.UnsupportedImageFormatException">Video frame must be 8 bpp grayscale image or 24/32 bpp color image.</exception>
            
        </member>
        <member name="M:AForge.Vision.Motion.GridMotionAreaProcessing.Reset">
             <summary>
             Reset internal state of motion processing algorithm.
             </summary>
             
             <remarks><para>The method allows to reset internal state of motion processing
             algorithm and prepare it for processing of next video stream or to restart
             the algorithm.</para></remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.GridMotionAreaProcessing.HighlightColor">
            <summary>
            Color used to highlight motion regions.
            </summary>
            
            <remarks>
            <para>Default value is set to <b>red</b> color.</para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.GridMotionAreaProcessing.HighlightMotionGrid">
             <summary>
             Highlight motion regions or not.
             </summary>
             
             <remarks><para>The property specifies if motion grid should be highlighted -
             if cell, which have motion level above the
             <see cref="P:AForge.Vision.Motion.GridMotionAreaProcessing.MotionAmountToHighlight">specified value</see>, should be highlighted.</para>
             
             <para>Default value is set to <see langword="true"/>.</para>
            
             <para><note>Turning the value on leads to additional processing time of video frame.</note></para>
             </remarks>
             
        </member>
        <member name="P:AForge.Vision.Motion.GridMotionAreaProcessing.MotionAmountToHighlight">
            <summary>
            Motion amount to highlight cell.
            </summary>
            
            <remarks><para>The property specifies motion level threshold for highlighting grid's
            cells. If motion level of a certain cell is higher than this value, then the cell
            is highlighted.</para>
            
            <para>Default value is set to <b>0.15</b>.</para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.GridMotionAreaProcessing.MotionGrid">
            <summary>
            Motion levels of each grid's cell.
            </summary>
            
            <remarks><para>The property represents an array of size
            <see cref="P:AForge.Vision.Motion.GridMotionAreaProcessing.GridHeight"/>x<see cref="P:AForge.Vision.Motion.GridMotionAreaProcessing.GridWidth"/>, which keeps motion level
            of each grid's cell. If certain cell has motion level equal to 0.2, then it
            means that this cell has 20% of changes.</para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.GridMotionAreaProcessing.GridWidth">
             <summary>
             Width of motion grid, [2, 64].
             </summary>
             
             <remarks><para>The property specifies motion grid's width - number of grid' columns.</para>
            
             <para>Default value is set to <b>16</b>.</para>
             </remarks>
             
        </member>
        <member name="P:AForge.Vision.Motion.GridMotionAreaProcessing.GridHeight">
             <summary>
             Height of motion grid, [2, 64].
             </summary>
             
             <remarks><para>The property specifies motion grid's height - number of grid' rows.</para>
            
             <para>Default value is set to <b>16</b>.</para>
             </remarks>
             
        </member>
        <member name="T:AForge.Vision.Motion.MotionAreaHighlighting">
            <summary>
            Motion processing algorithm, which highlights motion areas.
            </summary>
            
            <remarks><para>The aim of this motion processing algorithm is to highlight
            motion areas with grid pattern of the <see cref="P:AForge.Vision.Motion.MotionAreaHighlighting.HighlightColor">specified color</see>.
            </para>
            
            <para>Sample usage:</para>
            <code>
            // create motion detector
            MotionDetector detector = new MotionDetector(
                /* motion detection algorithm */,
                new MotionAreaHighlighting( ) );
            
            // continuously feed video frames to motion detector
            while ( ... )
            {
                // process new video frame
                detector.ProcessFrame( videoFrame );
            }
            </code>
            </remarks>
            
            <seealso cref="T:AForge.Vision.Motion.MotionDetector"/>
            <seealso cref="T:AForge.Vision.Motion.IMotionDetector"/>
            
        </member>
        <member name="M:AForge.Vision.Motion.MotionAreaHighlighting.#ctor">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.MotionAreaHighlighting"/> class.
            </summary>
            
        </member>
        <member name="M:AForge.Vision.Motion.MotionAreaHighlighting.#ctor(System.Drawing.Color)">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.MotionAreaHighlighting"/> class.
            </summary>
            
            <param name="highlightColor">Color used to highlight motion regions.</param>
            
        </member>
        <member name="M:AForge.Vision.Motion.MotionAreaHighlighting.ProcessFrame(AForge.Imaging.UnmanagedImage,AForge.Imaging.UnmanagedImage)">
             <summary>
             Process video and motion frames doing further post processing after
             performed motion detection.
             </summary>
             
             <param name="videoFrame">Original video frame.</param>
             <param name="motionFrame">Motion frame provided by motion detection
             algorithm (see <see cref="T:AForge.Vision.Motion.IMotionDetector"/>).</param>
             
             <remarks><para>Processes provided motion frame and highlights motion areas
             on the original video frame with <see cref="P:AForge.Vision.Motion.MotionAreaHighlighting.HighlightColor">specified color</see>.</para>
             </remarks>
             
             <exception cref="T:AForge.Imaging.InvalidImagePropertiesException">Motion frame is not 8 bpp image, but it must be so.</exception>
             <exception cref="T:AForge.Imaging.UnsupportedImageFormatException">Video frame must be 8 bpp grayscale image or 24/32 bpp color image.</exception>
            
        </member>
        <member name="M:AForge.Vision.Motion.MotionAreaHighlighting.Reset">
             <summary>
             Reset internal state of motion processing algorithm.
             </summary>
             
             <remarks><para>The method allows to reset internal state of motion processing
             algorithm and prepare it for processing of next video stream or to restart
             the algorithm.</para></remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.MotionAreaHighlighting.HighlightColor">
            <summary>
            Color used to highlight motion regions.
            </summary>
            
            <remarks>
            <para>Default value is set to <b>red</b> color.</para>
            </remarks>
            
        </member>
        <member name="T:AForge.Vision.Motion.MotionBorderHighlighting">
            <summary>
            Motion processing algorithm, which highlights border of motion areas.
            </summary>
            
            <remarks><para>The aim of this motion processing algorithm is to highlight
            borders of motion areas with the <see cref="P:AForge.Vision.Motion.MotionBorderHighlighting.HighlightColor">specified color</see>.
            </para>
            
            <para><note>The motion processing algorithm is supposed to be used only with motion detection
            algorithms, which are based on finding difference with background frame
            (see <see cref="T:AForge.Vision.Motion.SimpleBackgroundModelingDetector"/> and <see cref="T:AForge.Vision.Motion.CustomFrameDifferenceDetector"/>
            as simple implementations) and allow extract moving objects clearly.</note></para>
            
            <para>Sample usage:</para>
            <code>
            // create motion detector
            MotionDetector detector = new MotionDetector(
                /* motion detection algorithm */,
                new MotionBorderHighlighting( ) );
            
            // continuously feed video frames to motion detector
            while ( ... )
            {
                // process new video frame
                detector.ProcessFrame( videoFrame );
            }
            </code>
            </remarks>
            
            <seealso cref="T:AForge.Vision.Motion.MotionDetector"/>
            <seealso cref="T:AForge.Vision.Motion.IMotionDetector"/>
            
        </member>
        <member name="M:AForge.Vision.Motion.MotionBorderHighlighting.#ctor">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.MotionBorderHighlighting"/> class.
            </summary>
            
        </member>
        <member name="M:AForge.Vision.Motion.MotionBorderHighlighting.#ctor(System.Drawing.Color)">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.MotionBorderHighlighting"/> class.
            </summary>
            
            <param name="highlightColor">Color used to highlight motion regions.</param>
            
        </member>
        <member name="M:AForge.Vision.Motion.MotionBorderHighlighting.ProcessFrame(AForge.Imaging.UnmanagedImage,AForge.Imaging.UnmanagedImage)">
             <summary>
             Process video and motion frames doing further post processing after
             performed motion detection.
             </summary>
             
             <param name="videoFrame">Original video frame.</param>
             <param name="motionFrame">Motion frame provided by motion detection
             algorithm (see <see cref="T:AForge.Vision.Motion.IMotionDetector"/>).</param>
             
             <remarks><para>Processes provided motion frame and highlights borders of motion areas
             on the original video frame with <see cref="P:AForge.Vision.Motion.MotionBorderHighlighting.HighlightColor">specified color</see>.</para>
             </remarks>
            
             <exception cref="T:AForge.Imaging.InvalidImagePropertiesException">Motion frame is not 8 bpp image, but it must be so.</exception>
             <exception cref="T:AForge.Imaging.UnsupportedImageFormatException">Video frame must be 8 bpp grayscale image or 24/32 bpp color image.</exception>
            
        </member>
        <member name="M:AForge.Vision.Motion.MotionBorderHighlighting.Reset">
             <summary>
             Reset internal state of motion processing algorithm.
             </summary>
             
             <remarks><para>The method allows to reset internal state of motion processing
             algorithm and prepare it for processing of next video stream or to restart
             the algorithm.</para></remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.MotionBorderHighlighting.HighlightColor">
            <summary>
            Color used to highlight motion regions.
            </summary>
            
            <remarks>
            <para>Default value is set to <b>red</b> color.</para>
            </remarks>
            
        </member>
        <member name="T:AForge.Vision.Motion.MotionDetector">
             <summary>
             Motion detection wrapper class, which performs motion detection and processing.
             </summary>
            
             <remarks><para>The class serves as a wrapper class for
             <see cref="T:AForge.Vision.Motion.IMotionDetector">motion detection</see> and
             <see cref="T:AForge.Vision.Motion.IMotionProcessing">motion processing</see> algorithms, allowing to call them with
             single call. Unlike motion detection and motion processing interfaces, the class also
             provides additional methods for convenience, so the algorithms could be applied not
             only to <see cref="T:AForge.Imaging.UnmanagedImage"/>, but to .NET's <see cref="T:System.Drawing.Bitmap"/> class
             as well.</para>
             
             <para>In addition to wrapping of motion detection and processing algorthms, the class provides
             some additional functionality. Using <see cref="P:AForge.Vision.Motion.MotionDetector.MotionZones"/> property it is possible to specify
             set of rectangular zones to observe - only motion in these zones is counted and post procesed.</para>
             
             <para>Sample usage:</para>
             <code>
             // create motion detector
             MotionDetector detector = new MotionDetector(
                 new SimpleBackgroundModelingDetector( ),
                 new MotionAreaHighlighting( ) );
             
             // continuously feed video frames to motion detector
             while ( ... )
             {
                 // process new video frame and check motion level
                 if ( detector.ProcessFrame( videoFrame ) &gt; 0.02 )
                 {
                     // ring alarm or do somethng else
                 }
             }
             </code>
             </remarks>
            
        </member>
        <member name="M:AForge.Vision.Motion.MotionDetector.#ctor(AForge.Vision.Motion.IMotionDetector)">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.MotionDetector"/> class.
            </summary>
            
            <param name="detector">Motion detection algorithm to apply to each video frame.</param>
            
        </member>
        <member name="M:AForge.Vision.Motion.MotionDetector.#ctor(AForge.Vision.Motion.IMotionDetector,AForge.Vision.Motion.IMotionProcessing)">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.MotionDetector"/> class.
            </summary>
            
            <param name="detector">Motion detection algorithm to apply to each video frame.</param>
            <param name="processor">Motion processing algorithm to apply to each video frame after
            motion detection is done.</param>
            
        </member>
        <member name="M:AForge.Vision.Motion.MotionDetector.ProcessFrame(System.Drawing.Bitmap)">
            <summary>
            Process new video frame.
            </summary>
            
            <param name="videoFrame">Video frame to process (detect motion in).</param>
            
            <returns>Returns amount of motion, which is provided <see cref="P:AForge.Vision.Motion.IMotionDetector.MotionLevel"/>
            property of the <see cref="P:AForge.Vision.Motion.MotionDetector.MotionDetectionAlgorithm">motion detection algorithm in use</see>.</returns>
            
            <remarks><para>See <see cref="M:AForge.Vision.Motion.MotionDetector.ProcessFrame(AForge.Imaging.UnmanagedImage)"/> for additional details.</para>
            </remarks>
            
        </member>
        <member name="M:AForge.Vision.Motion.MotionDetector.ProcessFrame(System.Drawing.Imaging.BitmapData)">
             <summary>
             Process new video frame.
             </summary>
             
             <param name="videoFrame">Video frame to process (detect motion in).</param>
             
             <returns>Returns amount of motion, which is provided <see cref="P:AForge.Vision.Motion.IMotionDetector.MotionLevel"/>
             property of the <see cref="P:AForge.Vision.Motion.MotionDetector.MotionDetectionAlgorithm">motion detection algorithm in use</see>.</returns>
             
             <remarks><para>See <see cref="M:AForge.Vision.Motion.MotionDetector.ProcessFrame(AForge.Imaging.UnmanagedImage)"/> for additional details.</para>
             </remarks>
            
        </member>
        <member name="M:AForge.Vision.Motion.MotionDetector.ProcessFrame(AForge.Imaging.UnmanagedImage)">
            <summary>
            Process new video frame.
            </summary>
            
            <param name="videoFrame">Video frame to process (detect motion in).</param>
            
            <returns>Returns amount of motion, which is provided <see cref="P:AForge.Vision.Motion.IMotionDetector.MotionLevel"/>
            property of the <see cref="P:AForge.Vision.Motion.MotionDetector.MotionDetectionAlgorithm">motion detection algorithm in use</see>.</returns>
            
            <remarks><para>The method first of all applies motion detection algorithm to the specified video
            frame to calculate <see cref="P:AForge.Vision.Motion.IMotionDetector.MotionLevel">motion level</see> and
            <see cref="P:AForge.Vision.Motion.IMotionDetector.MotionFrame">motion frame</see>. After this it applies motion processing algorithm
            (if it was set) to do further post processing, like highlighting motion areas, counting moving
            objects, etc.</para>
            
            <para><note>In the case if <see cref="P:AForge.Vision.Motion.MotionDetector.MotionZones"/> property is set, this method will perform
            motion filtering right after motion algorithm is done and before passing motion frame to motion
            processing algorithm. The method does filtering right on the motion frame, which is produced
            by motion detection algorithm. At the same time the method recalculates motion level and returns
            new value, which takes motion zones into account (but the new value is not set back to motion detection
            algorithm' <see cref="P:AForge.Vision.Motion.IMotionDetector.MotionLevel"/> property).
            </note></para>
            </remarks>
            
        </member>
        <member name="M:AForge.Vision.Motion.MotionDetector.Reset">
            <summary>
            Reset motion detector to initial state.
            </summary>
            
            <remarks><para>The method resets motion detection and motion processing algotithms by calling
            their <see cref="M:AForge.Vision.Motion.IMotionDetector.Reset"/> and <see cref="M:AForge.Vision.Motion.IMotionProcessing.Reset"/> methods.</para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.MotionDetector.MotionDetectionAlgorithm">
             <summary>
             Motion detection algorithm to apply to each video frame.
             </summary>
            
             <remarks><para>The property sets motion detection algorithm, which is used by
             <see cref="M:AForge.Vision.Motion.MotionDetector.ProcessFrame(AForge.Imaging.UnmanagedImage)"/> method in order to calculate
             <see cref="P:AForge.Vision.Motion.IMotionDetector.MotionLevel">motion level</see> and
             <see cref="P:AForge.Vision.Motion.IMotionDetector.MotionFrame">motion frame</see>.
             </para></remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.MotionDetector.MotionProcessingAlgorithm">
            <summary>
            Motion processing algorithm to apply to each video frame after
            motion detection is done.
            </summary>
            
            <remarks><para>The property sets motion processing algorithm, which is used by
            <see cref="M:AForge.Vision.Motion.MotionDetector.ProcessFrame(AForge.Imaging.UnmanagedImage)"/> method after motion detection in order to do further
            post processing of motion frames. The aim of further post processing depends on
            actual implementation of the specified motion processing algorithm - it can be
            highlighting of motion area, objects counting, etc.
            </para></remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.MotionDetector.MotionZones">
            <summary>
            Set of zones to detect motion in.
            </summary>
            
            <remarks><para>The property keeps array of rectangular zones, which are observed for motion detection.
            Motion outside of these zones is ignored.</para>
            
            <para>In the case if this property is set, the <see cref="M:AForge.Vision.Motion.MotionDetector.ProcessFrame(AForge.Imaging.UnmanagedImage)"/> method
            will filter out all motion witch was detected by motion detection algorithm, but is not
            located in the specified zones.</para>
            </remarks>
            
        </member>
        <member name="T:AForge.Vision.Motion.SimpleBackgroundModelingDetector">
            <summary>
            Motion detector based on simple background modeling.
            </summary>
            
            <remarks><para>The class implements motion detection algorithm, which is based on
            difference of current video frame with modeled background frame.
            The <see cref="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.MotionFrame">difference frame</see> is thresholded and the
            <see cref="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.MotionLevel">amount of difference pixels</see> is calculated.
            To suppress stand-alone noisy pixels erosion morphological operator may be applied, which
            is controlled by <see cref="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.SuppressNoise"/> property.</para>
            
            <para><note>In the case if precise motion area's borders are required (for example,
            for further motion post processing), then <see cref="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.KeepObjectsEdges"/> property
            may be used to restore borders after noise suppression.</note></para>
            
            <para>As the first approximation of background frame, the first frame of video stream is taken.
            During further video processing the background frame is constantly updated, so it
            changes in the direction to decrease difference with current video frame (the background
            frame is moved towards current frame). See <see cref="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.FramesPerBackgroundUpdate"/>
            <see cref="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.MillisecondsPerBackgroundUpdate"/> properties, which control the rate of
            background frame update.</para>
            
            <para>Unlike <see cref="T:AForge.Vision.Motion.TwoFramesDifferenceDetector"/> motion detection algorithm, this algorithm
            allows to identify quite clearly all objects, which are not part of the background (scene) -
            most likely moving objects. And unlike <see cref="T:AForge.Vision.Motion.CustomFrameDifferenceDetector"/> motion
            detection algorithm, this algorithm includes background adaptation feature, which allows it
            to update its modeled background frame in order to take scene changes into account.</para>
            
            <para><note>Because of the adaptation feature of the algorithm, it may adopt
            to background changes, what <see cref="T:AForge.Vision.Motion.CustomFrameDifferenceDetector"/> algorithm can not do.
            However, if moving object stays on the scene for a while (so algorithm adopts to it and does
            not treat it as a new moving object any more) and then starts to move again, the algorithm may
            find two moving objects - the true one, which is really moving, and the false one, which does not (the
            place, where the object stayed for a while).</note></para>
            
            <para><note>The algorithm is not applicable to such cases, when moving object resides
            in camera's view most of the time (laptops camera monitoring a person sitting in front of it,
            for example). The algorithm is mostly supposed for cases, when camera monitors some sort
            of static scene, where moving objects appear from time to time - street, road, corridor, etc.
            </note></para>
            
            <para>Sample usage:</para>
            <code>
            // create motion detector
            MotionDetector detector = new MotionDetector(
                new SimpleBackgroundModelingDetector( ),
                new MotionAreaHighlighting( ) );
            
            // continuously feed video frames to motion detector
            while ( ... )
            {
                // process new video frame and check motion level
                if ( detector.ProcessFrame( videoFrame ) &gt; 0.02 )
                {
                    // ring alarm or do somethng else
                }
            }
            </code>
            </remarks>
            
            <seealso cref="T:AForge.Vision.Motion.MotionDetector"/>
            
        </member>
        <member name="M:AForge.Vision.Motion.SimpleBackgroundModelingDetector.#ctor">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.SimpleBackgroundModelingDetector"/> class.
            </summary>
        </member>
        <member name="M:AForge.Vision.Motion.SimpleBackgroundModelingDetector.#ctor(System.Boolean)">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.SimpleBackgroundModelingDetector"/> class.
            </summary>
            
            <param name="suppressNoise">Suppress noise in video frames or not (see <see cref="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.SuppressNoise"/> property).</param>
            
        </member>
        <member name="M:AForge.Vision.Motion.SimpleBackgroundModelingDetector.#ctor(System.Boolean,System.Boolean)">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.SimpleBackgroundModelingDetector"/> class.
            </summary>
            
            <param name="suppressNoise">Suppress noise in video frames or not (see <see cref="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.SuppressNoise"/> property).</param>
            <param name="keepObjectEdges">Restore objects edges after noise suppression or not (see <see cref="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.KeepObjectsEdges"/> property).</param>
            
        </member>
        <member name="M:AForge.Vision.Motion.SimpleBackgroundModelingDetector.ProcessFrame(AForge.Imaging.UnmanagedImage)">
             <summary>
             Process new video frame.
             </summary>
             
             <param name="videoFrame">Video frame to process (detect motion in).</param>
             
             <remarks><para>Processes new frame from video source and detects motion in it.</para>
             
             <para>Check <see cref="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.MotionLevel"/> property to get information about amount of motion
             (changes) in the processed frame.</para>
             </remarks>
            
        </member>
        <member name="M:AForge.Vision.Motion.SimpleBackgroundModelingDetector.Reset">
            <summary>
            Reset motion detector to initial state.
            </summary>
            
            <remarks><para>Resets internal state and variables of motion detection algorithm.
            Usually this is required to be done before processing new video source, but
            may be also done at any time to restart motion detection algorithm.</para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.DifferenceThreshold">
            <summary>
            Difference threshold value, [1, 255].
            </summary>
            
            <remarks><para>The value specifies the amount off difference between pixels, which is treated
            as motion pixel.</para>
            
            <para>Default value is set to <b>15</b>.</para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.MotionLevel">
            <summary>
            Motion level value, [0, 1].
            </summary>
            
            <remarks><para>Amount of changes in the last processed frame. For example, if value of
            this property equals to 0.1, then it means that last processed frame has 10% difference
            with modeled background frame.</para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.MotionFrame">
             <summary>
             Motion frame containing detected areas of motion.
             </summary>
             
             <remarks><para>Motion frame is a grayscale image, which shows areas of detected motion.
             All black pixels in the motion frame correspond to areas, where no motion is
             detected. But white pixels correspond to areas, where motion is detected.</para>
             
             <para><note>The property is set to <see langword="null"/> after processing of the first
             video frame by the algorithm.</note></para>
             </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.SuppressNoise">
            <summary>
            Suppress noise in video frames or not.
            </summary>
            
            <remarks><para>The value specifies if additional filtering should be
            done to suppress standalone noisy pixels by applying 3x3 erosion image processing
            filter. See <see cref="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.KeepObjectsEdges"/> property, if it is required to restore
            edges of objects, which are not noise.</para>
            
            <para>Default value is set to <see langword="true"/>.</para>
            
            <para><note>Turning the value on leads to more processing time of video frame.</note></para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.KeepObjectsEdges">
            <summary>
            Restore objects edges after noise suppression or not.
            </summary>
            
            <remarks><para>The value specifies if additional filtering should be done
            to restore objects' edges after noise suppression by applying 3x3 dilatation
            image processing filter.</para>
            
            <para>Default value is set to <see langword="false"/>.</para>
            
            <para><note>Turning the value on leads to more processing time of video frame.</note></para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.FramesPerBackgroundUpdate">
            <summary>
            Frames per background update, [1, 50].
            </summary>
            
            <remarks><para>The value controls the speed of modeled background adaptation to
            scene changes. After each specified amount of frames the background frame is updated
            in the direction to decrease difference with current processing frame.</para>
            
            <para>Default value is set to <b>2</b>.</para>
            
            <para><note>The property has effect only in the case if <see cref="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.MillisecondsPerBackgroundUpdate"/>
            property is set to <b>0</b>. Otherwise it does not have effect and background
            update is managed according to the <see cref="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.MillisecondsPerBackgroundUpdate"/>
            property settings.</note></para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.MillisecondsPerBackgroundUpdate">
            <summary>
            Milliseconds per background update, [0, 5000].
            </summary>
            
            <remarks><para>The value represents alternate way of controlling the speed of modeled
            background adaptation to scene changes. The value sets number of milliseconds, which
            should elapse between two consequent video frames to result in background update
            for one intensity level. For example, if this value is set to 100 milliseconds and
            the amount of time elapsed between two last video frames equals to 350, then background
            frame will be update for 3 intensity levels in the direction to decrease difference
            with current video frame (the remained 50 milliseconds will be added to time difference
            between two next consequent frames, so the accuracy is preserved).</para>
            
            <para>Unlike background update method controlled using <see cref="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.FramesPerBackgroundUpdate"/>
            method, the method guided by this property is not affected by changes
            in frame rates. If, for some reasons, a video source starts to provide delays between
            frames (frame rate drops down), the amount of background update still stays consistent.
            When background update is controlled by this property, it is always possible to estimate
            amount of time required to change, for example, absolutely black background (0 intensity
            values) into absolutely white background (255 intensity values). If value of this
            property is set to 100, then it will take approximately 25.5 seconds for such update
            regardless of frame rate.</para>
            
            <para><note>Background update controlled by this property is slightly slower then
            background update controlled by <see cref="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.FramesPerBackgroundUpdate"/> property,
            so it has a bit greater impact on performance.</note></para>
            
            <para><note>If this property is set to 0, then corresponding background updating
            method is not used (turned off), but background update guided by
            <see cref="P:AForge.Vision.Motion.SimpleBackgroundModelingDetector.FramesPerBackgroundUpdate"/> property is used.</note></para>
            
            <para>Default value is set to <b>0</b>.</para>
            </remarks>
            
        </member>
        <member name="T:AForge.Vision.Motion.TwoFramesDifferenceDetector">
            <summary>
            Motion detector based on two continues frames difference.
            </summary>
            
            <remarks><para>The class implements the simplest motion detection algorithm, which is
            based on difference of two continues frames. The <see cref="P:AForge.Vision.Motion.TwoFramesDifferenceDetector.MotionFrame">difference frame</see>
            is thresholded and the <see cref="P:AForge.Vision.Motion.TwoFramesDifferenceDetector.MotionLevel">amount of difference pixels</see> is calculated.
            To suppress stand-alone noisy pixels erosion morphological operator may be applied, which
            is controlled by <see cref="P:AForge.Vision.Motion.TwoFramesDifferenceDetector.SuppressNoise"/> property.</para>
            
            <para>Although the class may be used on its own to perform motion detection, it is preferred
            to use it in conjunction with <see cref="T:AForge.Vision.Motion.MotionDetector"/> class, which provides additional
            features and allows to use moton post processing algorithms.</para>
            
            <para>Sample usage:</para>
            <code>
            // create motion detector
            MotionDetector detector = new MotionDetector(
                new TwoFramesDifferenceDetector( ),
                new MotionAreaHighlighting( ) );
            
            // continuously feed video frames to motion detector
            while ( ... )
            {
                // process new video frame and check motion level
                if ( detector.ProcessFrame( videoFrame ) &gt; 0.02 )
                {
                    // ring alarm or do somethng else
                }
            }
            </code>
            </remarks>
            
            <seealso cref="T:AForge.Vision.Motion.MotionDetector"/>
            
        </member>
        <member name="M:AForge.Vision.Motion.TwoFramesDifferenceDetector.#ctor">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.TwoFramesDifferenceDetector"/> class.
            </summary>
            
        </member>
        <member name="M:AForge.Vision.Motion.TwoFramesDifferenceDetector.#ctor(System.Boolean)">
            <summary>
            Initializes a new instance of the <see cref="T:AForge.Vision.Motion.TwoFramesDifferenceDetector"/> class.
            </summary>
            
            <param name="suppressNoise">Suppress noise in video frames or not (see <see cref="P:AForge.Vision.Motion.TwoFramesDifferenceDetector.SuppressNoise"/> property).</param>
            
        </member>
        <member name="M:AForge.Vision.Motion.TwoFramesDifferenceDetector.ProcessFrame(AForge.Imaging.UnmanagedImage)">
            <summary>
            Process new video frame.
            </summary>
            
            <param name="videoFrame">Video frame to process (detect motion in).</param>
            
            <remarks><para>Processes new frame from video source and detects motion in it.</para>
            
            <para>Check <see cref="P:AForge.Vision.Motion.TwoFramesDifferenceDetector.MotionLevel"/> property to get information about amount of motion
            (changes) in the processed frame.</para>
            </remarks>
            
        </member>
        <member name="M:AForge.Vision.Motion.TwoFramesDifferenceDetector.Reset">
            <summary>
            Reset motion detector to initial state.
            </summary>
            
            <remarks><para>Resets internal state and variables of motion detection algorithm.
            Usually this is required to be done before processing new video source, but
            may be also done at any time to restart motion detection algorithm.</para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.TwoFramesDifferenceDetector.DifferenceThreshold">
            <summary>
            Difference threshold value, [1, 255].
            </summary>
            
            <remarks><para>The value specifies the amount off difference between pixels, which is treated
            as motion pixel.</para>
            
            <para>Default value is set to <b>15</b>.</para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.TwoFramesDifferenceDetector.MotionLevel">
            <summary>
            Motion level value, [0, 1].
            </summary>
            
            <remarks><para>Amount of changes in the last processed frame. For example, if value of
            this property equals to 0.1, then it means that last processed frame has 10% difference
            with previous frame.</para>
            </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.TwoFramesDifferenceDetector.MotionFrame">
             <summary>
             Motion frame containing detected areas of motion.
             </summary>
             
             <remarks><para>Motion frame is a grayscale image, which shows areas of detected motion.
             All black pixels in the motion frame correspond to areas, where no motion is
             detected. But white pixels correspond to areas, where motion is detected.</para>
             
             <para><note>The property is set to <see langword="null"/> after processing of the first
             video frame by the algorithm.</note></para>
             </remarks>
            
        </member>
        <member name="P:AForge.Vision.Motion.TwoFramesDifferenceDetector.SuppressNoise">
            <summary>
            Suppress noise in video frames or not.
            </summary>
            
            <remarks><para>The value specifies if additional filtering should be
            done to suppress standalone noisy pixels by applying 3x3 erosion image processing
            filter.</para>
            
            <para>Default value is set to <see langword="true"/>.</para>
            
            <para><note>Turning the value on leads to more processing time of video frame.</note></para>
            </remarks>
            
        </member>
        <member name="T:Accord.Vision.Tracking.MatchingTracker">
            <summary>
              Template matching object tracker.
            </summary>
            
            <remarks>
              The matching tracker will track the object presented in the search window
              of the first frame given to the tracker. To reset the tracker and start
              tracking another object, one can call the Reset method, then set the search
              window around a new object of interest present the image containing the new
              object to the tracker.
            </remarks>
            
        </member>
        <member name="T:Accord.Vision.Tracking.IObjectTracker">
            <summary>
              Object tracker interface.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.IObjectTracker.ProcessFrame(AForge.Imaging.UnmanagedImage)">
            <summary>
              Process a new video frame.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.IObjectTracker.TrackingObject">
            <summary>
              Gets the current location of the object being tracked.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.IObjectTracker.Extract">
            <summary>
              Gets or sets a value indicating whether the tracker should
              extract the object image from the source. The extracted image
              should be stored in <see cref="P:Accord.Vision.Tracking.IObjectTracker.TrackingObject"/>.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.MatchingTracker.#ctor">
            <summary>
              Constructs a new <see cref="T:Accord.Vision.Tracking.MatchingTracker"/> object tracker.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.MatchingTracker.ProcessFrame(AForge.Imaging.UnmanagedImage)">
            <summary>
              Process a new video frame.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.MatchingTracker.Reset">
            <summary>
              Resets this instance.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.MatchingTracker.SearchWindow">
            <summary>
              Gets or sets the current search window.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.MatchingTracker.TrackingObject">
            <summary>
              Gets the current location of the object being tracked.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.MatchingTracker.Threshold">
            <summary>
              Gets or sets the similarity threshold to 
              determine when the object has been lost.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.MatchingTracker.Extract">
            <summary>
              Gets or sets a value indicating whether the tracker should
              extract the object image from the source. The extracted image
              should be stored in <see cref="P:Accord.Vision.Tracking.MatchingTracker.TrackingObject"/>.
            </summary>
            
        </member>
        <member name="T:Accord.Vision.Tracking.CamshiftMode">
            <summary>
              Modes for the Camshift Tracker.
            </summary>
            
        </member>
        <member name="F:Accord.Vision.Tracking.CamshiftMode.RGB">
            <summary>
              By choosing RGB, the tracker will process raw high-intensity RGB values.
            </summary>
            
        </member>
        <member name="F:Accord.Vision.Tracking.CamshiftMode.HSL">
            <summary>
              By choosing HSL, the tracker will perform a RGB-to-HSL conversion and use the Hue value instead.
            </summary>
            
        </member>
        <member name="F:Accord.Vision.Tracking.CamshiftMode.Mixed">
            <summary>
              By choosing Mixed, the tracker will use HSL with some lightness information.
            </summary>
            
        </member>
        <member name="T:Accord.Vision.Tracking.Camshift">
            <summary>
              Continuously Adaptive Mean Shift (Camshift) Object Tracker
            </summary>
            <remarks>
            <para>
              Camshift stands for "Continuously Adaptive Mean Shift". It combines the basic
              Mean Shift algorithm with an adaptive region-sizing step. The kernel is a step
              function applied to a probability map. The probability of each image pixel is
              based on color using a method called histogram backprojection.</para>
            <para>
              The implementation of this code has used Gary Bradski's original publication,
              the OpenCV Library and the FaceIt implementation as references. The OpenCV
              library is distributed under a BSD license. FaceIt is distributed under a MIT
              license. The original licensing terms for FaceIt are described in the source
              code and in the Copyright.txt file accompanying the framework.</para>  
              
            <para>
              References:
              <list type="bullet">
                <item><description>
                  G.R. Bradski, Computer video face tracking for use in a perceptual user interface,
                  Intel Technology Journal, Q2 1998. Available on:
                  <a href="ftp://download.intel.com/technology/itj/q21998/pdf/camshift.pdf">
                  ftp://download.intel.com/technology/itj/q21998/pdf/camshift.pdf </a></description></item>
                <item><description>
                  R. Hewitt, Face tracking project description: Camshift Algorithm. Available on:
                  <a href="http://www.robinhewitt.com/research/track/camshift.html">
                  http://www.robinhewitt.com/research/track/camshift.html </a></description></item>
                <item><description>
                  OpenCV Computer Vision Library. Available on:
                  <a href="http://sourceforge.net/projects/opencvlibrary/">
                  http://sourceforge.net/projects/opencvlibrary/ </a></description></item>
                <item><description>
                  FaceIt object tracking in Flash AS3. Available on:
                  <a href="http://www.libspark.org/browser/as3/FaceIt">
                  http://www.libspark.org/browser/as3/FaceIt </a></description></item>
             </list></para>  
            </remarks>
            
        </member>
        <member name="M:Accord.Vision.Tracking.Camshift.#ctor">
            <summary>
              Constructs a new Camshift tracking algorithm.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.Camshift.#ctor(AForge.Imaging.UnmanagedImage,System.Drawing.Rectangle)">
            <summary>
              Constructs a new Camshift tracking algorithm.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.Camshift.#ctor(System.Drawing.Rectangle)">
            <summary>
              Constructs a new Camshift tracking algorithm.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.Camshift.#ctor(AForge.Imaging.UnmanagedImage,System.Drawing.Rectangle,Accord.Vision.Tracking.CamshiftMode)">
            <summary>
              Constructs a new Camshift tracking algorithm.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.Camshift.#ctor(System.Drawing.Rectangle,Accord.Vision.Tracking.CamshiftMode)">
            <summary>
              Constructs a new Camshift tracking algorithm.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.Camshift.Reset">
            <summary>
              Resets the object tracking algorithm.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.Camshift.GetBackprojection">
            <summary>
              Generates a image of the histogram back projection
            </summary>
        </member>
        <member name="M:Accord.Vision.Tracking.Camshift.GetBackprojection(System.Drawing.Imaging.PixelFormat)">
            <summary>
              Generates a image of the histogram backprojection
            </summary>
        </member>
        <member name="M:Accord.Vision.Tracking.Camshift.GetBackprojection(System.Drawing.Imaging.PixelFormat,System.Drawing.Rectangle)">
            <summary>
              Generates a image of the histogram backprojection
            </summary>
        </member>
        <member name="M:Accord.Vision.Tracking.Camshift.GetBackprojection(AForge.Imaging.UnmanagedImage,System.Drawing.Rectangle)">
            <summary>
              Generates a image of the histogram backprojection
            </summary>
        </member>
        <member name="M:Accord.Vision.Tracking.Camshift.ProcessFrame(AForge.Imaging.UnmanagedImage)">
            <summary>
              Processes a new video frame.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.Camshift.camshift(AForge.Imaging.UnmanagedImage)">
            <summary>
              Camshift algorithm
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.Camshift.meanShift(AForge.Imaging.UnmanagedImage)">
            <summary>
              Mean shift algorithm
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.Camshift.computeHistogramRatio(System.Single[],System.Single[],System.Single[])">
            <summary>
              Computes the ratio histogram between to histograms.
            </summary>
            
            <remarks>
              http://www.robinhewitt.com/research/track/backproject.html
            </remarks>
            
        </member>
        <member name="M:Accord.Vision.Tracking.Camshift.generateBackprojectionMap(AForge.Imaging.UnmanagedImage,System.Single[])">
            <summary>
              Image histogram back-projection.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.Camshift.createHistogram(AForge.Imaging.UnmanagedImage,System.Drawing.Rectangle)">
            <summary>
              Creates a color histogram discarding low intensity colors
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.Camshift.checkSteadiness">
            <summary>
              Checks for aberrant fluctuations in the tracking object.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.Camshift.SearchWindow">
            <summary>
              Gets or sets the current search window.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.Camshift.AspectRatio">
            <summary>
              Gets or sets the desired window aspect ratio.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.Camshift.Mode">
            <summary>
              Gets or sets the mode of operation for this tracker.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.Camshift.HslSaturationRange">
            <summary>
              If using HSL mode, specifies the operational saturation range for the tracker.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.Camshift.HslLightnessRange">
            <summary>
              If using HSL mode, specifies the operational lightness range for the tracker.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.Camshift.TrackingObject">
            <summary>
              Gets the location of the object being tracked.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.Camshift.Extract">
            <summary>
              Gets or sets a value indicating whether the tracker
              should extract the object image from the source. The
              extracted image will be available in <see cref="P:Accord.Vision.Tracking.TrackingObject.Image"/>.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.Camshift.Map">
            <summary>
              Probability map
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.Camshift.Conservative">
            <summary>
              Gets or sets whether the algorithm should scan only the
              active window or the entire image for histogram ratio.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.Camshift.Smooth">
            <summary>
              Gets or sets a value indicating whether the angular
              movements should be smoothed using a moving average.
            </summary>
            <value><c>true</c> to smooth angular movements; otherwise, <c>false</c>.</value>
            
        </member>
        <member name="P:Accord.Vision.Tracking.Camshift.IsSteady">
            <summary>
              Gets whether the tracking object is
              showing little variation of fluctuation.
            </summary>
            <value><c>true</c> if the tracking object is steady; otherwise, <c>false</c>.</value>
            
        </member>
        <member name="T:Accord.Vision.Tracking.HslBlobTracker">
            <summary>
              Blob object tracker.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.HslBlobTracker.#ctor">
            <summary>
              Initializes a new instance of the <see cref="T:Accord.Vision.Tracking.HslBlobTracker"/> class.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.HslBlobTracker.#ctor(AForge.Imaging.Filters.HSLFiltering)">
            <summary>
              Initializes a new instance of the <see cref="T:Accord.Vision.Tracking.HslBlobTracker"/> class.
            </summary>
            
            <param name="filter">The filter.</param>
            
        </member>
        <member name="M:Accord.Vision.Tracking.HslBlobTracker.ProcessFrame(AForge.Imaging.UnmanagedImage)">
            <summary>
            Process a new video frame.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.HslBlobTracker.Reset">
            <summary>
            Resets this instance.
            </summary>
        </member>
        <member name="P:Accord.Vision.Tracking.HslBlobTracker.MaxWidth">
            <summary>
            Gets or sets the maximum width of tracked objects.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.HslBlobTracker.MaxHeight">
            <summary>
            Gets or sets the maximum height of tracked objects.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.HslBlobTracker.MinWidth">
            <summary>
            Gets or sets the minimum width of tracked objects.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.HslBlobTracker.MinHeight">
            <summary>
            Gets or sets the minimum height of tracked objects.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.HslBlobTracker.Extract">
            <summary>
              Gets or sets a value indicating whether the tracker
              should extract the object image from the source. The
              extracted image will be available in <see cref="P:Accord.Vision.Tracking.TrackingObject.Image"/>.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.HslBlobTracker.ComputeOrientation">
            <summary>
              Gets or sets whether the tracker should compute blob's orientation.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.HslBlobTracker.Filter">
            <summary>
              Gets the HSL filter used in color segmentation.
            </summary>
            
            <value>The HSL filter used in segmentation.</value>
            
        </member>
        <member name="P:Accord.Vision.Tracking.HslBlobTracker.FilterImage">
            <summary>
              Gets the HSL filtered image.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.HslBlobTracker.TrackingObject">
            <summary>
            Gets the current location of the object being tracked.
            </summary>
            
        </member>
        <member name="T:Accord.Vision.Tracking.AxisOrientation">
            <summary>
              Axis orientation.
            </summary>
            
        </member>
        <member name="F:Accord.Vision.Tracking.AxisOrientation.Horizontal">
            <summary>
              Horizontal axis.
            </summary>
            
        </member>
        <member name="F:Accord.Vision.Tracking.AxisOrientation.Vertical">
            <summary>
              Vertical axis.
            </summary>
            
        </member>
        <member name="T:Accord.Vision.Tracking.TrackingObject">
            <summary>
              Tracking object to represent an object in a scene.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.TrackingObject.#ctor">
            <summary>
              Constructs a new tracking object.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.TrackingObject.#ctor(AForge.IntPoint)">
            <summary>
              Constructs a new tracking object.
            </summary>
            
            <param name="center">The center of gravity of the object.</param>
            
        </member>
        <member name="M:Accord.Vision.Tracking.TrackingObject.#ctor(System.Drawing.Rectangle,AForge.IntPoint,System.Single)">
            <summary>
              Constructs a new tracking object.
            </summary>
            
            <param name="angle">The angle of orientation for the object.</param>
            <param name="center">The center of gravity of the object.</param>
            <param name="rectangle">The rectangle containing the object.</param>
            
        </member>
        <member name="M:Accord.Vision.Tracking.TrackingObject.#ctor(System.Drawing.Rectangle,System.Single)">
            <summary>
              Constructs a new tracking object.
            </summary>
            
            <param name="rectangle">The rectangle containing the object.</param>
            <param name="angle">The angle of the object.</param>
            
        </member>
        <member name="M:Accord.Vision.Tracking.TrackingObject.GetAxis">
            <summary>
              Gets two points defining the horizontal axis of the object.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.TrackingObject.GetAxis(Accord.Vision.Tracking.AxisOrientation)">
            <summary>
              Gets two points defining the axis of the object.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.TrackingObject.Reset">
            <summary>
              Resets this tracking object.
            </summary>
            
        </member>
        <member name="M:Accord.Vision.Tracking.TrackingObject.System#ICloneable#Clone">
            <summary>
              Creates a new object that is a copy of the current instance.
            </summary>
            
            <returns>
              A new object that is a copy of this instance.
            </returns>
            
        </member>
        <member name="M:Accord.Vision.Tracking.TrackingObject.Clone(System.Boolean)">
            <summary>
              Creates a new object that is a copy of the current instance.
            </summary>
            
            <returns>
              A new object that is a copy of this instance.
            </returns>
            
            <param name="excludeImage">Pass true to not include
              the <see cref="P:Accord.Vision.Tracking.TrackingObject.Image"/> in the copy object.</param>
            
        </member>
        <member name="P:Accord.Vision.Tracking.TrackingObject.Tag">
            <summary>
            Gets or sets an user-defined tag associated with this object.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.TrackingObject.Rectangle">
            <summary>
              Gets or sets the rectangle containing the object.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.TrackingObject.Center">
            <summary>
              Gets or sets the center of gravity of the object 
              relative to the original image from where it has 
              been extracted.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.TrackingObject.Image">
            <summary>
              Gets or sets the object's extracted image.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.TrackingObject.IsEmpty">
            <summary>
            Gets a value indicating whether the object is empty.
            </summary>
            
            <value><c>true</c> if this instance is empty; otherwise, <c>false</c>.</value>
            
        </member>
        <member name="P:Accord.Vision.Tracking.TrackingObject.Area">
            <summary>
              Gets the area of the object.
            </summary>
            
        </member>
        <member name="P:Accord.Vision.Tracking.TrackingObject.Angle">
            <summary>
              Gets or sets the angle of the object.
            </summary>
            
        </member>
    </members>
</doc>
